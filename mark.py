# -*- coding: utf-8 -*-
"""mark.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tI0Pd41os4VyT81Du2OgaHiHMgw_zlrU
"""

import gspread
import pandas as pd
from google.auth import default
from google.colab import auth

# Authenticate with Google
auth.authenticate_user()
creds, _ = default()

# Connect to Google Sheets
gc = gspread.authorize(creds)

# Open the shared Google Sheet by URL or name
SHEET_URL = "https://docs.google.com/spreadsheets/d/1Zz3e4QPMA4rStYARsrvF_Vxoz36f7OYfYIJchZXazAk/edit?usp=drive_link"
spreadsheet = gc.open_by_url(SHEET_URL)

# Select the worksheet (first sheet)
worksheet = spreadsheet.sheet1

# Convert to Pandas DataFrame
df = pd.DataFrame(worksheet.get_all_records())

# Display DataFrame
print(df.head())

df_filtered = df[["gene_symbol", "hgnc_id", "disease_label", "mondo_id"]][0:500]

def read_google_sheet(sheet_url, sheet_name='Sheet1'):
  """Reads a Google Sheet into a pandas DataFrame.

  Args:
    sheet_url: The URL of the Google Sheet.
    sheet_name: The name of the worksheet to read from.
      Defaults to 'Sheet1'.

  Returns:
    A pandas DataFrame containing the sheet data.
  """

  # Authenticate with Google
  creds, _ = default()
  gc = gspread.authorize(creds)

  # Open the spreadsheet by URL
  spreadsheet = gc.open_by_url(sheet_url)

  # Select the worksheet by name
  worksheet = spreadsheet.worksheet(sheet_name)

  # Get all data as a list of lists
  data = worksheet.get_all_values()

  # Convert to a pandas DataFrame
  df = pd.DataFrame(data[1:], columns=data[0])

  return df

df = read_google_sheet("https://docs.google.com/spreadsheets/d/1m6x2s9Tagb7Y2J0dS2KzQ7DXVNPocBpGqJkx74a06-Y/edit?usp=drive_link", "Sampled_Data_10")

# Display DataFrame
print(df.head())

df_filtered = df[0:500]

df_filtered.head()

import requests

def fetch_clingen_gene_disease_interaction(gene_symbol):
    """Fetch gene-disease interaction summary from ClinGen API."""
    url = f"https://search.clinicalgenome.org/kb/gene-validity?search={gene_symbol}"

    try:

        response = requests.get(url, timeout=100)
        response.raise_for_status()
        print("hey")
        print("response.json() = " + response.json())
        data = response.json()

        print("hyello")

        # Debugging: Print the API response to verify structure
        print("API Response:", data)

        # Extract relevant summary fields
        interactions = []
        for record in data.get("records", []):  # Iterate through multiple records
            interactions.append({
                "Gene": gene_symbol,
                "Disease": record.get("diseaseLabel", "N/A"),
                "Classification": record.get("classification", "N/A"),
                "Mode of Inheritance": record.get("modeOfInheritance", "N/A"),
                "Evidence Level": record.get("evidenceLevel", "N/A"),
                "Last Evaluated": record.get("lastEvaluated", "N/A"),
                "Curator": record.get("curator", "N/A"),
            })

            print(interactions.gene_symbol)

        return interactions if interactions else [{"Gene": gene_symbol, "Disease": "No interactions found"}]

    except requests.exceptions.RequestException as e:
         print(f"Error fetching ClinGen data for {gene_symbol}: {e}")
         return [{"Gene": gene_symbol, "Disease": "Error retrieving data"}]

# Example usage
gene = "MTOR"
print(fetch_clingen_gene_disease_interaction(gene))

import requests
import time

def fetch_abstracts_semanticscholar(gene, disease, hgnc_id, mondo_id, max_results=5, retries=3, delay=5):
    """Fetch abstracts from Semantic Scholar for a gene-disease relationship."""

    query = f"{gene} {disease}"
    url = f"https://api.semanticscholar.org/graph/v1/paper/search?query={query}&fields=title,abstract,year,journal,externalIds&limit={max_results}"

    for attempt in range(retries):
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()  # Raise error if request fails

            data = response.json()
            articles = []

            for result in data.get("data", []):
                articles.append({
                    "gene_symbol": gene,
                    "hgnc_id": hgnc_id,
                    "disease_label": disease,
                    "mondo_id": mondo_id,
                    "title": result.get("title", "N/A"),
                    "abstract": result.get("abstract", "N/A"),
                    "DOI": result.get("externalIds", {}).get("DOI", "N/A"),
                })

            return articles

        except requests.exceptions.RequestException as e:
            print(f"Error fetching data for {gene}-{disease}: {e}")
            if attempt < retries - 1:
                time.sleep(delay)
            else:
                return []

# Collect abstracts for all gene-disease relationships
all_articles = []
for _, row in df_filtered.iterrows():
    gene_symbol = row["gene_symbol"]
    disease_label = row["disease_label"]
    hgnc_id = row["hgnc_id"]
    mondo_id = row["mondo_id"]

    articles = fetch_abstracts_semanticscholar(gene_symbol, disease_label, hgnc_id, mondo_id, max_results=10)
    all_articles.extend(articles)

# Convert results into a DataFrame
df_abstracts = pd.DataFrame(all_articles)

print(f"Saved {len(df_abstracts)} abstracts to gene_disease_abstracts.csv")

# Save to CSV file
df_abstracts.to_csv("gene_disease_abstracts.csv", index=False)

!ls /content/gene_disease_abstracts.csv

from google.colab import files

# Download the CSV file
files.download("gene_disease_abstracts.csv")

df_abstracts.head()

df[['disease_label','mondo_id']]

# Create a dictionary from the original dataset
mapping_dict = df.set_index("disease_label")["mondo_id"].to_dict()

print(mapping_dict)
# Map MONDO IDs in the new dataset
df_abstracts["mondo_id"] = df_abstracts["Disease_Label"].map(mapping_dict)

df_abstracts.head()